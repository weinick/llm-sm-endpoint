{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c261e5f4-17a8-40da-beb9-599f1717e0fe",
   "metadata": {},
   "source": [
    "### 1. 安装HuggingFace 并下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02785614-9268-41c8-85a5-d579490edbbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install huggingface-hub -Uqq -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install -U sagemaker==2.235.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install tiktoken  -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install boto3==1.34.101  -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba24701-47db-4107-9a6c-1667038d0054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ./LLM_qwen_25_int8_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d5ddf",
   "metadata": {},
   "source": [
    "## 如果是海外区域，可以直接从HF下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bd7ee-16a3-4f5a-8857-8bbba83eb9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import snapshot_download\n",
    "#from pathlib import Path\n",
    "#local_model_path = Path(\"./LLM_qwen_int4_model\")\n",
    "#local_model_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3469632-4174-4df4-a7d1-ef167561c626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = \"Qwen/Qwen-7B-Chat-Int4\"\n",
    "# commit_hash = \"b725fe596dce755fe717c5b15e5c8243d5474f66\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8abc5-a58e-40e2-b1e6-fbf48307c716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# snapshot_download(repo_id=model_name, revision=commit_hash, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a644dd-d048-4629-b51c-38913b9a684f",
   "metadata": {},
   "source": [
    "## 如果是中国区，从modelscope下载比较快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f191be3-111b-41eb-a858-a12012c3bee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./LLM_qwen_25_int8_model\")\n",
    "#local_model_path = Path(\"./LLM_qwen_25_int4_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "#model_name = \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\"\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\"\n",
    "# commit_hash = \"v1.1.4\"\n",
    "commit_hash = \"master\"\n",
    "\n",
    "snapshot_download(model_name, revision=commit_hash, cache_dir=local_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d666c79-b039-4258-ac3b-46b19e63c3b8",
   "metadata": {},
   "source": [
    "### 2. 把模型拷贝到S3为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9431deb-6359-442d-847b-1563f8dd3854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "#from sagemaker.djl_inference.model import DJLModel\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "as_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd8f16-ae7c-48bf-8e52-1a15425fa74d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_model_prefix = \"LLM-RAG/workshop/LLM_qwen_25_int8_stream_model\"  # folder where model checkpoint will go\n",
    "#s3_model_prefix = \"LLM-RAG/workshop/LLM_qwen_25_int4_stream_model\"\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/Qwen/*\"))[0]\n",
    "#s3_code_prefix = \"LLM-RAG/workshop/LLM_qwen_25_int4_stream_deploy_code_bak\"\n",
    "s3_code_prefix = \"LLM-RAG/workshop/LLM_qwen_25_int8_stream_deploy_code_bak\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067292c9-c066-4649-a61f-b460a24da584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#把Notebook本地缓存的模型文件拷贝到S3\n",
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}\n",
    "s3_path = f\"s3://{bucket}/{s3_model_prefix}/\"\n",
    "print(f\"option.s3url ==> {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b70c3-90f1-4175-95bf-568bafbcd383",
   "metadata": {},
   "source": [
    "### 3. 模型部署准备（entrypoint脚本，容器镜像，服务配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c4277-4480-42c6-aee6-1fbcca94eb82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 中国区需要替换为下面的image_uri，通过DJL框架支持http的endpoint接口、弹性扩展以及高性能高吞吐的推理服务。\n",
    "# 通过deepspeed深度学习框架提供了模型并行化的能力，可以通过多GPU解决长Prompt推理显存溢出的问题以及模型超过单GPU显存不足无法部署的问题\n",
    "#inference_image_uri = (\n",
    "#    f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.29.0-lmi11.0.0-cu124\"\n",
    "#)\n",
    "\n",
    "#inference_image_uri = (\n",
    "#    f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.27.0-deepspeed0.12.6-cu121\"\n",
    "#)\n",
    "\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-lmi\",\n",
    "    region=sess.boto_session.region_name,\n",
    "     version=\"0.29.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082bb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p LLM_qwen_25_int8_stream_deploy_code_bak\n",
    "!mkdir -p LLM_qwen_25_int4_stream_deploy_code_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile LLM_qwen_25_int4_stream_deploy_code_bak/serving.properties\n",
    "#%%writefile LLM_qwen_25_int8_stream_deploy_code_bak/serving.properties\n",
    "engine=Python\n",
    "#通过下面参数修改模型的Tensor并行度，从而控制模型可以部署到几块GPU卡\n",
    "option.tensor_parallel_degree=1\n",
    "\n",
    "option.rolling_batch=vllm\n",
    "# Adjust the following based on model size and instance type\n",
    "option.max_rolling_batch_size=32\n",
    "option.max_model_len=4096\n",
    "#option.tensor_parallel_degree=2\n",
    "option.enable_streaming=True\n",
    "option.predict_timeout=240\n",
    "#option.predict_timeout=360\n",
    "#option.trust_remote_code=true\n",
    "#option.s3url = S3PATH\n",
    "option.model_id = S3PATH\n",
    "#当使用替换deepspeed后的djl-lmi框架的镜像时，如果serving.properties与config.json不再同一层级目录保存，必须使用option.model_id。如果使用option.s3url则会在模型部署时报错找不到config文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i \"s|option.model_id = S3PATH|option.model_id = {s3_path}|\" LLM_qwen_25_int4_stream_deploy_code_bak/serving.properties\n",
    "#!sed -i \"s|option.model_id = S3PATH|option.model_id = {s3_path}|\" LLM_qwen_25_int8_stream_deploy_code_bak/serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13039174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#存储推理代码并打包成模型文件\n",
    "!rm model.tar.gz\n",
    "#!cd LLM_qwen_25_int8_stream_deploy_code_bak && rm -rf \".ipynb_checkpoints\"\n",
    "!cd LLM_qwen_25_int4_stream_deploy_code_bak && rm -rf \".ipynb_checkpoints\"\n",
    "#!tar czvf model.tar.gz LLM_qwen_25_int8_stream_deploy_code_bak\n",
    "!tar czvf model.tar.gz LLM_qwen_25_int4_stream_deploy_code_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75580e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#上传模型文件到S3\n",
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853daa-b8a3-4485-8c0a-64bf83e93a18",
   "metadata": {},
   "source": [
    "### 4. 创建模型 & 创建endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sagemaker中创建模型\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = name_from_base(f\"qwen-25-14B-stream-int4\") #Note: Need to specify model_name\n",
    "#model_name = name_from_base(f\"qwen-25-14B-stream-int8\")\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sagemaker中创建模型端点配置文件\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "variant_name = f\"{model_name}-variant\"\n",
    "\n",
    "#Note: ml.g4dn.2xlarge 也可以选择\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"ModelName\": model_name,\n",
    "            #\"InstanceType\": \"ml.g4dn.12xlarge\",\n",
    "            #\"InstanceType\": \"ml.g4dn.2xlarge\",\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3adb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建Sagemaker endpoint\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ca396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[\"EndpointName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#endpoint创建成功后创建AS的伸缩规则\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "pp.pprint(response)\n",
    "\n",
    "#此处务必使用创建endpoint config时指定的product variant name\n",
    "resource_id = 'endpoint/' + endpoint_name + '/variant/' + variant_name\n",
    "\n",
    "#在AS中注册一个sagemaker使用的扩展目标\n",
    "response = as_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2\n",
    ")\n",
    "\n",
    "#配置扩展策略 - SageMakerVariantInvocationsPerInstance Metric\n",
    "response = as_client.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id, # Endpoint name \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5, # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n",
    "        'PredefinedMetricSpecification': {\n",
    "            #'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance', # is the average number of times per minute that each instance for a variant is invoked. \n",
    "            #收集1分钟内的指标\n",
    "            'PredefinedMetricType': 'SageMakerVariantConcurrentRequestsPerModelHighResolution',\n",
    "        },\n",
    "        'ScaleInCooldown': 600, # The cooldown period helps you prevent your Auto Scaling group from launching or terminating \n",
    "                                # additional instances before the effects of previous activities are visible. \n",
    "                                # You can configure the length of time based on your instance startup time or other application needs.\n",
    "                                # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \n",
    "        'ScaleOutCooldown': 300 # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        \n",
    "        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled. \n",
    "                            # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1959e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查询当前的扩展策略\n",
    "response = as_client.describe_scaling_policies(\n",
    "    ServiceNamespace='sagemaker'\n",
    ")\n",
    "\n",
    "for i in response['ScalingPolicies']:\n",
    "    print('')\n",
    "    pp.pprint(i['PolicyName'])\n",
    "    print('')\n",
    "    if('TargetTrackingScalingPolicyConfiguration' in i):\n",
    "        pp.pprint(i['TargetTrackingScalingPolicyConfiguration']) \n",
    "    else:\n",
    "        pp.pprint(i['StepScalingPolicyConfiguration'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义扩展目标的服务命名空间、资源ID和可扩展维度\n",
    "service_namespace = 'sagemaker'\n",
    "scalable_dimention = 'sagemaker:variant:DesiredInstanceCount'\n",
    "# 删除扩展目标\n",
    "response = as_client.deregister_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimention\n",
    ")\n",
    "\n",
    "# 打印响应\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5447e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d985b427-3959-46f7-9a50-5a2b45e2d513",
   "metadata": {},
   "source": [
    "### 5. 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240200a",
   "metadata": {},
   "source": [
    "## None stream - 逻辑问题测试endpoint状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#prompts1 = \"写一首春天的诗\"\n",
    "prompts1 = \"\"\"你现在是口袋奇兵(TopWar:Battle Game)的专属智能客服TopWarBot，你是一个非常专业的游戏客服，，请严格确理解的根据反引号中的资料提取相关信息，回答指挥官的各种问题，不可联想推测反引号中没有的信息,不可提供带有岐义和模棱两可的回答。\n",
    "```\n",
    "问题: 钛蓝金币转换成钻石的比例\n",
    "回答: 按照1：1的比例i进行转换，1个钛蓝金币=1个钻石\n",
    "\n",
    "问题: 金币\n",
    "回答: 1、金币的定义：金币是《口袋奇兵》世界中的通用货币\n",
    "2、金币的获得方法：在游戏内可以通过税收中心/金币收割机进行获取。需要放置金矿生产金币，同时有部分装饰是可以提高金币产量。税收中心只需要20个当前玩家等级民居即可达到之前造满民居的收益。新版本的金币自动收割机是需要10个当前等级的金矿。\n",
    "3、金币的用途：建筑升级、装备制造与改造等大部分游戏功能均需要消耗一定数量的金币\n",
    "\n",
    "问题: 远征行动中如何查看小地图与大地图\n",
    "回答: 在远征行动主页面点击游戏正上方的小地图，可以查看当前关卡的信息，在点击右上角的“地图”就可以观看远征行动大地图信息\n",
    "\n",
    "问题: 数值单位\n",
    "回答: 游戏的数值显示单位为\n",
    "k→m→b→t→aa→bb→cc→dd→ee→ff→gg→hh依次递进，1000进一个单位，比如1000=1k，1000k=1m，1000m=1b，1000b=1t，1000t=1aa，1000aa=1bb，1000bb=1cc。\n",
    "```\n",
    "\n",
    "指挥官: 金币单位t大还是b大\n",
    "TopWarBot: \n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\":\"user\", \"content\": prompts1}]\n",
    "# endpoint_name = 'qwen-stream-int4-2024-11-07-04-05-10-872-endpoint'\n",
    "start = time.time()\n",
    "\n",
    "response = predictor.predict({\n",
    "    \"messages\": messages,\n",
    "     \"max_tokens\":4096,\n",
    "     \"repetition_penalty\": 1.05,\n",
    "     \"temperature\": 0.7,\n",
    "     \"top_p\": 0.8,\n",
    "     \"top_k\": 20\n",
    "    }\n",
    ")\n",
    "# text = str(response, \"utf-8\")\n",
    "# print(response)\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "#response_model = smr_client.invoke_endpoint(\n",
    "#            EndpointName=endpoint_name,\n",
    "#            Body=json.dumps(\n",
    "#            {\n",
    "#                \"inputs\" : prompts1,\n",
    "#                \"messages\": messages,\n",
    "#                \"parameters\": parameters\n",
    "#            }\n",
    "#            ),\n",
    "#            ContentType=\"application/json\",\n",
    "#        )\n",
    "\n",
    "#resp = response_model['Body'].read()\n",
    "print (f\"\\ntime:{time.time()-start} s\")\n",
    "#print(resp.decode('utf8'))\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0950a",
   "metadata": {},
   "source": [
    "## 测试流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a398fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ad9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "\n",
    "class MessageTokenIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "\n",
    "            # print(line)\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                full_line = line[:-1].decode(\"utf-8\")\n",
    "                # print(full_line)\n",
    "                line_data = json.loads(full_line.lstrip(\"data:\").rstrip(\"/n\"))\n",
    "                return line_data[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "            chunk = next(self.byte_iterator)\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "        \n",
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "        CustomAttributes='accept_eula=false'\n",
    "    )\n",
    "    return response_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"世界上最高的山峰是哪座\"\n",
    "#messages = [\n",
    "#    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#    {\"role\": \"user\", \"content\": prompt}\n",
    "#]\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "payload= {\n",
    "    \"messages\": messages,\n",
    "     \"max_tokens\":4096,\n",
    "     \"temperature\": 0.7,\n",
    "     \"top_p\": 0.8,\n",
    "    \"stream\": \"true\"\n",
    "    }\n",
    "\n",
    "response_stream = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "# print_response_stream(response_stream)\n",
    "for token in MessageTokenIterator(response_stream[\"Body\"]):\n",
    "    # pass\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#请自行输入用于验证的真实业务prompt\n",
    "prompts = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入tiktoken用于后续计算context的token数量\n",
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1366d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算prompt的token数量\n",
    "prompt = \"\".join([prompts])\n",
    "token = encoding.encode(prompts)\n",
    "print('context Token:', len(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b581f6d",
   "metadata": {},
   "source": [
    "## 串形多个请求提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69056b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#运行多轮推理计算token产生的平局速度，包括产生1000个token的时间以及每秒产生的token时间\n",
    "#start_time = time.time()\n",
    "\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "rounds = 2\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompts}\n",
    "]\n",
    "payload= {\n",
    "    \"messages\": messages,\n",
    "     \"max_tokens\":4096,\n",
    "     \"temperature\": 0.7,\n",
    "     \"top_p\": 0.8,\n",
    "    \"stream\": \"true\"\n",
    "    }\n",
    "\n",
    "for _ in range(rounds):\n",
    "    start = time.time()\n",
    "    #sequence = generate_reponse(prompts1)\n",
    "    token_len = 0\n",
    "    response_stream = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "    # print_response_stream(response_stream)\n",
    "    for token in MessageTokenIterator(response_stream[\"Body\"]):\n",
    "    # pass\n",
    "        \n",
    "        print(token, end=\"\")\n",
    "        token_len += len(token)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    # print(\"\\n\")\n",
    "\n",
    "    #token_len = 0\n",
    "    #sequence = sequence.replace(\"<|endoftext|>\",\"\")\n",
    "    #print(sequence, end='\\n')\n",
    "    # token = encoding.encode(sequence.replace(\"<|endoftext|>\",\"\"))\n",
    "    #token = encoding.encode(sequence)\n",
    "\n",
    "    #print('Generate Token:', len(token))\n",
    "    print('Generate Token:', token_len)\n",
    "    \n",
    "    print('Time per 1K Token:', (end-start)*1024/token_len)\n",
    "    print('Token per 1s:', token_len/(end-start))\n",
    "    print(\"-\"*100)\n",
    "    total_time += (end-start)\n",
    "    total_tokens += token_len\n",
    "\n",
    "print('avg Token:',total_tokens/rounds)\n",
    "print('avg Time per 1K Token:', (total_time)*1024/total_tokens)\n",
    "print('avg Token per 1s:', total_tokens/(total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45b443",
   "metadata": {},
   "source": [
    "## 并行多个请求提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6545df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_gen_stream(prompts):\n",
    "#运行多轮推理计算token产生的平局速度，包括产生1000个token的时间以及每秒产生的token时间\n",
    "#start_time = time.time()\n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    first_token_latency = 0\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompts}\n",
    "    ]\n",
    "    payload= {\n",
    "        \"messages\": messages,\n",
    "         \"max_tokens\":4096,\n",
    "         \"temperature\": 0.7,\n",
    "         \"top_p\": 0.8,\n",
    "        \"stream\": \"true\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        start = time.time()\n",
    "        #sequence = generate_reponse(prompts1)\n",
    "        token_len = 0\n",
    "        response_stream = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "        # print_response_stream(response_stream)\n",
    "        for token in MessageTokenIterator(response_stream[\"Body\"]):\n",
    "        # pass\n",
    "            if first_token_latency == 0:\n",
    "                first_token_latency = time.time() - start\n",
    "            #print(token, end=\"\")\n",
    "            token_len += len(token)\n",
    "    \n",
    "        end = time.time()\n",
    "        res_time = end-start\n",
    "       \n",
    "        #print('Generate Token:', token_len)\n",
    "    \n",
    "        #print('Time per 1K Token:', (end-start)*1024/token_len)\n",
    "        #print('Token per 1s:', token_len/(end-start))\n",
    "        #print(\"-\"*100)\n",
    "        \n",
    "        #total_time += (end-start)\n",
    "        #total_tokens += token_len\n",
    "        return (\n",
    "            first_token_latency, res_time, token_len\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "    \n",
    "def submit_concurrent_requests(num_requests, prompts):\n",
    "    # 存储指标数据\n",
    "    request_ids = []\n",
    "    first_token_latencies = []\n",
    "    total_times = []\n",
    "    total_tokens = []\n",
    "    time_per_1k_tokens = []\n",
    "    tokens_per_second = []\n",
    "    prompts_list = [prompts] * num_requests\n",
    "    total_failure = 0\n",
    "    \n",
    "    #并发20是单G5.2xlarge的极限（100请求出现1个失败），如果prompt的token数量减少(目前输入输出一共约3000)，可以支持到24\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        futures = [executor.submit(res_gen_stream, prompts) for prompt in prompts_list]\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        #return results\n",
    "\n",
    "        for i, result in enumerate(results, start=1):\n",
    "            if result:\n",
    "                first_token_latency, res_time, token_len = result\n",
    "                print(f\"Request {i}:\")\n",
    "                print(f\"First Token Latency: {first_token_latency:.2f} seconds\")\n",
    "                print(f\"Total Time: {res_time:.2f} seconds\")\n",
    "                print(f\"Total Tokens: {token_len}\")\n",
    "                print(f\"Time per 1K Token: {res_time*1024/token_len} seconds\")\n",
    "                print(f\"Token per 1s: {token_len/res_time} tokens\")\n",
    "        \n",
    "                # 存储指标数据\n",
    "                request_ids.append(i)\n",
    "                first_token_latencies.append(first_token_latency)\n",
    "                total_times.append(res_time)\n",
    "                total_tokens.append(token_len)\n",
    "                time_per_1k_tokens.append(res_time*1024/token_len)\n",
    "                tokens_per_second.append(token_len/res_time)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Request {i} failed.\")\n",
    "                total_failure += 1\n",
    "            print(\"-\" * 50)\n",
    "    print(f\"Total failure: {total_failure}\")\n",
    "    # 创建 DataFrame\n",
    "    data = {\n",
    "        'Request ID': request_ids,\n",
    "        'First Token Latency (s)': first_token_latencies,\n",
    "        'Total Time (s)': total_times,\n",
    "        'Total Tokens': total_tokens,\n",
    "        'Time per 1K Token (s)': time_per_1k_tokens,\n",
    "        'Token per 1s': tokens_per_second\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "     # 将 DataFrame 写入 CSV 文件\n",
    "    #df.to_csv('qwen25-concurrent-test-result/metrics.csv', index=False)\n",
    "\n",
    "    # 创建折线图\n",
    "    for column in df.columns[1:]:\n",
    "        chart = pd.DataFrame({\n",
    "            'Request ID': df['Request ID'],\n",
    "            column: df[column]\n",
    "        }).set_index('Request ID').plot(kind='line', title=column, figsize=(12, 6))\n",
    "        #chart.figure.savefig(f'qwen25-concurrent-test-result/{column}.png')\n",
    "    \n",
    "    \n",
    "# 示例用法\n",
    "num_requests = 100\n",
    "submit_concurrent_requests(num_requests, prompts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7264a5",
   "metadata": {},
   "source": [
    "## qwen15+HF框架与qwen25+vllm的性能对比结果\n",
    "\n",
    "1. 测试目标\n",
    "    \n",
    "    通过同样token数的问题验证在相同的A10单卡上通过2.5版本以及vllm实现的性能提升\n",
    "    \n",
    "2. 测试用例\n",
    "    \n",
    "    约2700个输入token的背景prompt包含相同的问题，提交10次作业，对比每秒产生的token数变化以及1K token产生的用时。模型采用int8和int4两种GPTQ量化技术\n",
    "    \n",
    "3. 测试发现\n",
    "   \n",
    "   a.切换为vllm后由于模型中已经实现了相应的get model和inference方法，无需使用自定义的model.py文件\n",
    "    \n",
    "   b.中国区不推荐使用0.30的djl-lmi的镜像，部署时会出现异常\n",
    "    \n",
    "   c.测试具体结果如下\n",
    "   \n",
    "        \n",
    "    Qwen2.5(INT8)+vllm\n",
    "        \n",
    "        avg Token: 228.9\n",
    "        avg Time per 1K Token: 17.00462971258601\n",
    "        avg Token per 1s: 60.218894342761516\n",
    "        \n",
    "    Qwen2.5(INT4)+vllm\n",
    "        \n",
    "        avg Token: 211.4\n",
    "        avg Time per 1K Token: 17.20364607970672\n",
    "        avg Token per 1s: 60.218894342761516\n",
    "        \n",
    "    Qwen1.5+HF     \n",
    "    \n",
    "        avg Token: 666.0\n",
    "        avg Time per 1K Token: 37.71944202503285\n",
    "        avg Token per 1s: 27.147803494028707\n",
    "        \n",
    "4. 测试结论\n",
    "\n",
    "    a.Qwen2.5+VLLM相比Qwen1.5+HF框架在A10显卡可以提升一倍的性能\n",
    "    \n",
    "    b.Qwen2.5生成答案内容更加简洁，平均在200 token左右，但1.5生成600个token答案\n",
    "    \n",
    "    c.Int8相比Int4没有对性能带来提升\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e0e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f63d8f4",
   "metadata": {},
   "source": [
    "## 删除资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name {endpoint_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint-config --endpoint-config-name {endpoint_config_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc311a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-model --model-name {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477d3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
